{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo đường dẫn chung để đọc utils\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef57e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF parsing\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTTextLine, LTChar\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Text processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Embedding & Vector search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Utils\n",
    "from utils.load_chunks_json import load_chunks_from_json\n",
    "from utils.save_chunks_json import save_chunks_to_json\n",
    "from utils.bm25 import bm25_tokenize, text_to_sparse_vector_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d91c33",
   "metadata": {},
   "source": [
    "# ***Tạo hàm xử lý các chunk***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdc8f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chapters(text):\n",
    "    \"\"\"\n",
    "    Phân tích 1 dòng chương/tiêu đề, hỗ trợ các định dạng:\n",
    "    - 'Chương 1 Tiêu đề...'\n",
    "    - 'Chương nhập môn Tiêu đề...'\n",
    "    - 'I. Tiêu đề...'\n",
    "    - '1. Tiêu đề...'\n",
    "\n",
    "    Trả về tuple: (chapter_number, chapter_title)\n",
    "    Nếu không khớp format, trả về (, )\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"\"\"^\\s*\n",
    "        (                                         # Nhóm 1: định danh chương\n",
    "            Chương\\s+(?:nhập\\s+môn|\\d+)           # 'Chương nhập môn' hoặc 'Chương <số>'\n",
    "            |[IVXLCDM]+\\.                         # Số La Mã có dấu chấm (I., II., ...)\n",
    "            |\\d+\\.?                                # Số thường, có thể có hoặc không dấu chấm\n",
    "        )\n",
    "        \\s*(.*)                                    # Nhóm 2: tiêu đề còn lại\n",
    "        \"\"\", re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "    \n",
    "    line = text.strip()\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        chapter_number = match.group(1).strip().rstrip('.') or \"\"\n",
    "        chapter_title = match.group(2).strip() or \"\"\n",
    "\n",
    "        if (chapter_number and len(chapter_number) > 1) or (chapter_title and len(chapter_title) > 1):\n",
    "            return chapter_number, chapter_title\n",
    "        else:\n",
    "            return \"\", text\n",
    "    else:\n",
    "        return \"\", text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "250c7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Chuyển chuỗi thành lowercase, xóa dấu, và xóa toàn bộ khoảng trắng.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Bỏ dấu\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # Xóa toàn bộ khoảng trắng (space, tab, newline)\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c7c3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm dùng để chuyển đổi data từ RAW chunks sang Dense Chunks\n",
    "def raw_to_dense_chunk(\n",
    "    chapter_line,\n",
    "    section_line,\n",
    "    sub_section_line,\n",
    "    content_line,\n",
    "    max_chars=2048,\n",
    "    model= None\n",
    "):\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chars,\n",
    "        chunk_overlap=300,  \n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    content_chunks = splitter.split_text(content_line)\n",
    "\n",
    "    # Xử lý metadata như cũ\n",
    "    chapter_number, chapter_title = parse_chapters(chapter_line) if chapter_line else (\"\", \"\")\n",
    "    section_number, section_title = parse_chapters(section_line) if section_line else (\"\", \"\")\n",
    "    subsection_number, subsection_title = parse_chapters(sub_section_line) if sub_section_line else (\"\", \"\")\n",
    "\n",
    "    if 'MỤC TIÊU' in section_line:\n",
    "        chunk_type = 'TARGET'\n",
    "    elif 'NỘI DUNG ÔN TẬP VÀ THẢO LUẬN' in section_line:\n",
    "        chunk_type = 'EXERCISES'\n",
    "    else:\n",
    "        chunk_type = 'THEORY'\n",
    "\n",
    "    result = []\n",
    "    for i, content_piece in enumerate(content_chunks):\n",
    "        content = content_piece.strip()\n",
    "        chunk = {\n",
    "            \"id\": f\"LSD_{normalize_text(chapter_number) if chapter_number else 0}_{section_number if section_number else 0}_{subsection_number if subsection_number else 0}_{i}\",\n",
    "            \"values\": model.encode(content).tolist(),\n",
    "            \"metadata\": {\n",
    "                \"subject\": \"Lịch sử Đảng Cộng Sản Việt Nam\",\n",
    "                \"chapter\": chapter_number,\n",
    "                \"chapter_title\": chapter_title,\n",
    "                \"section\": section_number,\n",
    "                \"section_title\": section_title,\n",
    "                \"subsection\": subsection_number,\n",
    "                \"subsection_title\": subsection_title,\n",
    "                \"content\": content,\n",
    "                \"tokens\": len(content_piece),\n",
    "                \"type\": chunk_type\n",
    "            }\n",
    "        }\n",
    "        result.append(chunk)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4243f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm dùng để chuyển đổi data từ RAW chunks sang Sparse Chunks\n",
    "def raw_to_sparse_chunk(\n",
    "    chapter_line,\n",
    "    section_line,\n",
    "    sub_section_line,\n",
    "    content_line,\n",
    "    bm25=None,\n",
    "    vocabulary=None,\n",
    "    max_chars=2048\n",
    "):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chars,\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    content_chunks = splitter.split_text(content_line)\n",
    "\n",
    "    # Metadata parsing\n",
    "    chapter_number, chapter_title = parse_chapters(chapter_line) if chapter_line else (\"\", \"\")\n",
    "    section_number, section_title = parse_chapters(section_line) if section_line else (\"\", \"\")\n",
    "    subsection_number, subsection_title = parse_chapters(sub_section_line) if sub_section_line else (\"\", \"\")\n",
    "\n",
    "    if 'MỤC TIÊU' in section_line:\n",
    "        chunk_type = 'TARGET'\n",
    "    elif 'NỘI DUNG ÔN TẬP VÀ THẢO LUẬN' in section_line:\n",
    "        chunk_type = 'EXERCISES'\n",
    "    else:\n",
    "        chunk_type = 'THEORY'\n",
    "\n",
    "    result = []\n",
    "    for i, content_piece in enumerate(content_chunks):\n",
    "        content = content_piece.strip()\n",
    "        sparse_vector = text_to_sparse_vector_bm25(content, bm25, vocabulary)\n",
    "\n",
    "        chunk = {\n",
    "            \"id\": f\"LSD_{normalize_text(chapter_number) if chapter_number else 0}_{section_number if section_number else 0}_{subsection_number if subsection_number else 0}_{i}\",\n",
    "            \"sparse_values\": sparse_vector,\n",
    "            \"metadata\": {\n",
    "                \"subject\": \"Lịch sử Đảng Cộng Sản Việt Nam\",\n",
    "                \"chapter\": chapter_number,\n",
    "                \"chapter_title\": chapter_title,\n",
    "                \"section\": section_number,\n",
    "                \"section_title\": section_title,\n",
    "                \"subsection\": subsection_number,\n",
    "                \"subsection_title\": subsection_title,\n",
    "                \"content\": content,\n",
    "                \"tokens\": len(content_piece),\n",
    "                \"type\": chunk_type\n",
    "            }\n",
    "        }\n",
    "        result.append(chunk)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3af3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_chunks_LSD(pdf_path, total_pages):\n",
    "    \"\"\"\n",
    "    Trích xuất các đoạn nội dung theo chương - mục - mục con mà chưa split theo embedding.\n",
    "    Trả về danh sách các dict: {\"chapter\": ..., \"section\": ..., \"sub_section\": ..., \"content\": ...}\n",
    "    \"\"\"\n",
    "    raw_chunks = []\n",
    "    is_content = False\n",
    "\n",
    "    curr_chapter_line = \"\"\n",
    "    curr_section_line = \"\"\n",
    "    curr_sub_section_line = \"\"\n",
    "    curr_content_line = \"\"\n",
    "    temp = [\"1\", \"2\", \"3\", \"4\"]\n",
    "    count = 0 \n",
    "    for page_layout in extract_pages(pdf_path, page_numbers=range(2, total_pages)):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                for text_line in element:\n",
    "                    if isinstance(text_line, LTTextLine):\n",
    "                        line_text = \"\"\n",
    "                        for obj in text_line:\n",
    "                            if isinstance(obj, LTChar):\n",
    "                                char = obj.get_text()\n",
    "                                size = obj.size\n",
    "\n",
    "                                if size < 10 and char.isdigit():\n",
    "                                    continue\n",
    "\n",
    "                                line_text += char\n",
    "                        line_text = line_text.strip()\n",
    "                        # line_text = text_line.get_text().strip()\n",
    "                        first_obj = next(iter(text_line), None)\n",
    "\n",
    "                        if isinstance(first_obj, LTChar):\n",
    "                            font = first_obj.fontname\n",
    "                            text_size = first_obj.size\n",
    "\n",
    "                            if text_size == 14:\n",
    "                                if is_content:\n",
    "                                    raw_chunks.append({\n",
    "                                        \"chapter\": curr_chapter_line.strip(),\n",
    "                                        \"section\": curr_section_line.strip(),\n",
    "                                        \"sub_section\": curr_sub_section_line.strip(),\n",
    "                                        \"content\": curr_content_line.strip()\n",
    "                                    })\n",
    "                                    curr_chapter_line = \"\"\n",
    "                                    curr_section_line = \"\"\n",
    "                                    curr_sub_section_line = \"\"\n",
    "                                    curr_content_line = \"\"\n",
    "                                    is_content = False\n",
    "                                curr_chapter_line += line_text + \" \"\n",
    "\n",
    "                            elif 12.9 < text_size < 14:\n",
    "                                if \"Bold\" in font and \"Italic\" not in font:\n",
    "                                    if is_content and line_text not in [\"\", \" \", \"c\", \",\"]:\n",
    "                                        for temp_item in temp:\n",
    "                                            if temp_item in curr_section_line:\n",
    "                                                curr_content_line = curr_section_line + curr_content_line\n",
    "                                                curr_section_line = temp_item\n",
    "                                                curr_sub_section_line = \"\"\n",
    "                                        if \"KẾT LUẬN\" in curr_section_line.upper():\n",
    "                                            curr_chapter_line = \"KẾT LUẬN\"\n",
    "                                            curr_section_line = \"\"\n",
    "                                            curr_sub_section_line = \"\"\n",
    "                                        raw_chunks.append({\n",
    "                                            \"chapter\": curr_chapter_line.strip(),\n",
    "                                            \"section\": curr_section_line.strip(),\n",
    "                                            \"sub_section\": curr_sub_section_line.strip(),\n",
    "                                            \"content\": curr_content_line.strip()\n",
    "                                        })\n",
    "                                        curr_section_line = \"\"\n",
    "                                        curr_sub_section_line = \"\"\n",
    "                                        curr_content_line = \"\"\n",
    "                                        is_content = False\n",
    "                                    curr_section_line += line_text + \" \"\n",
    "\n",
    "                                elif \"Bold\" in font and \"Italic\" in font:\n",
    "                                    if is_content and line_text.strip() != \"\":\n",
    "                                        raw_chunks.append({\n",
    "                                            \"chapter\": curr_chapter_line.strip(),\n",
    "                                            \"section\": curr_section_line.strip(),\n",
    "                                            \"sub_section\": curr_sub_section_line.strip(),\n",
    "                                            \"content\": curr_content_line.strip()\n",
    "                                        })\n",
    "                                        curr_sub_section_line = \"\"\n",
    "                                        curr_content_line = \"\"\n",
    "                                        is_content = False\n",
    "                                    curr_sub_section_line += line_text + \" \"\n",
    "\n",
    "                                else:\n",
    "                                    is_content = True\n",
    "                                    curr_content_line += line_text + \" \"\n",
    "    if is_content and curr_content_line.strip():\n",
    "        raw_chunks.append({\n",
    "            \"chapter\": curr_chapter_line.strip(),\n",
    "            \"section\": curr_section_line.strip(),\n",
    "            \"sub_section\": curr_sub_section_line.strip(),\n",
    "            \"content\": curr_content_line.strip()\n",
    "        })\n",
    "\n",
    "    return raw_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6f7d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_embedding(raw_chunks, embedding_model):\n",
    "    \"\"\"\n",
    "    Nhận vào danh sách raw_chunks, sau đó chuyển danh sách các raw chunks sang Dense chunks\n",
    "    \"\"\"\n",
    "    final_chunks = []\n",
    "    for chunk in raw_chunks:\n",
    "        split_chunks = raw_to_dense_chunk(\n",
    "            chunk['chapter'],\n",
    "            chunk['section'],\n",
    "            chunk['sub_section'],\n",
    "            chunk['content'],\n",
    "            max_chars=2048,\n",
    "            model=embedding_model\n",
    "        )\n",
    "        final_chunks.extend(split_chunks)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff407543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_sparse(raw_chunks, bm25, vocabulary):\n",
    "    \"\"\"\n",
    "    Nhận vào danh sách raw_chunks, sau đó chuyển danh sách các raw chunks sang Dense chunks\n",
    "    \"\"\"\n",
    "    final_chunks = []\n",
    "    for chunk in raw_chunks:\n",
    "        split_chunks = raw_to_sparse_chunk(\n",
    "            chunk['chapter'],\n",
    "            chunk['section'],\n",
    "            chunk['sub_section'],\n",
    "            chunk['content'],\n",
    "            bm25=bm25,\n",
    "            vocabulary=vocabulary\n",
    "        )\n",
    "        final_chunks.extend(split_chunks)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cea99b",
   "metadata": {},
   "source": [
    "# ***Main***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aceac8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./lich_su_dang.pdf\"\n",
    "\n",
    "# # Lấy số trang trong file PDF\n",
    "# reader = PdfReader(pdf_path)\n",
    "# total_pages = len(reader.pages)\n",
    "# # print(\"Số trang trong PDF:\", total_pages)\n",
    "\n",
    "# #Trích xuất rawchunks từ file PDF\n",
    "# LSD_raw_chunk = extract_raw_chunks_LSD(pdf_path, total_pages)\n",
    "\n",
    "# #Tạo Densve Vector từ RAW chunks\n",
    "# embedding_model = SentenceTransformer(\"AITeamVN/Vietnamese_Embedding\")\n",
    "# embedding_model.max_seq_length = 2048\n",
    "# LSD_dense_chunks = chunk_with_embedding(LSD_raw_chunk, embedding_model)\n",
    "\n",
    "# #Tạo Sparse Vector từ RAW chunks\n",
    "# corpus_texts = [chunk[\"content\"] for chunk in LSD_raw_chunk]\n",
    "# tokenized_corpus = [bm25_tokenize(text) for text in corpus_texts]\n",
    "# bm25 = BM25Okapi(tokenized_corpus)\n",
    "# vocabulary = list(bm25.idf.keys())\n",
    "# LSD_sparse_chunks = chunk_with_sparse(LSD_raw_chunk, bm25, vocabulary)\n",
    "\n",
    "## Lưu các Vector vừa tạo ra file Json\n",
    "# save_chunks_to_json(LSD_raw_chunk, r\"./Lich_Su_Dang_raw.json\")\n",
    "# save_chunks_to_json(LSD_dense_chunks, r\"./Lich_Su_Dang_Dense.json\")\n",
    "# save_chunks_to_json(LSD_sparse_chunks, r\"./Lich_Su_Dang_Sparse.json\")\n",
    "\n",
    "#Đọc các file json ra thành chunks, chuẩn bị upsert lên Database\n",
    "LSD_raw_chunk = load_chunks_from_json(r\"./Lich_Su_Dang_raw.json\")\n",
    "LSD_dense_chunks = load_chunks_from_json(r\"./Lich_Su_Dang_Dense.json\")\n",
    "LSD_sparse_chunks = load_chunks_from_json(r\"./Lich_Su_Dang_Sparse.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
